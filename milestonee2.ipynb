{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "144e05dd-f59e-40bf-bf5b-f4ef509f166f",
   "metadata": {},
   "source": [
    "# Milestone 2: Model Enhancement & Tuning\n",
    "\n",
    "Loan Approval Prediction - Milestone 2: Model Enhancement & Tuning\n",
    "\n",
    "This covers:\n",
    "\n",
    "- Hyperparameter Tuning: Finding optimal model configuration for best performance.\n",
    "- Cross-Validation: Robust evaluation technique to prevent data leakage and overfitting.\n",
    "- Regularization: Techniques to prevent overfitting by penalizing complex models.\n",
    "- Model Architectures: Experimenting with different algorithms to find the best performer.\n",
    "- Ensemble Methods: Combining multiple models to improve predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a089636-a1b5-4b6c-9ba1-e430918db45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libaries\n",
    "import pandas as pd #Used for data manipulation and analysis, especially for working with tabular data (DataFrames and Series).\n",
    "import numpy as np #Provides support for numerical operations, including arrays, matrices, and mathematical functions.\n",
    "import matplotlib.pyplot as plt #Used for creating visualizations, such as line plots, bar charts, scatter plots, etc.\n",
    "import seaborn as sns #It provides attractive statistical graphics like heatmaps, violin plots, and boxplots.\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler #mports two tools from Scikit-learn's preprocessing module\n",
    "#LabelEncoder: Converts categorical string labels into numeric form.\n",
    "#StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
    "from sklearn.feature_selection import SelectKBest, f_classif #Imports tools for feature selection\n",
    "from sklearn.decomposition import PCA #Imports Principal Component Analysis (PCA) from Scikit-learn.\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f54717e-514f-42bc-9225-6b0e9ecdda32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# random_state=42 ensures reproducibility\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Fixed random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3914b78-6f34-478a-a4ea-9caadc564a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Hyperparameter Tuning using GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\"\"\"\n",
    "    Find optimal hyperparameters for different models.\n",
    "    \n",
    "    Why we do this:\n",
    "    - Default parameters are rarely optimal for specific datasets\n",
    "    - Proper tuning can significantly improve model performance\n",
    "    - Different problems require different model configurations\n",
    "    - Helps balance bias-variance tradeoff\n",
    "    \"\"\"\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],  # Number of trees (more trees = better but slower)\n",
    "    'max_depth': [None, 5, 10], # Tree depth (None = unlimited, 5/10 = restricts depth)\n",
    "    'min_samples_split': [2, 5] # Min samples to split a node (2 = default, 5 = conservative)\n",
    "}\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X_train, y_train)\n",
    "# Fit the grid search to training data\n",
    "print('Best parameters:', grid.best_params_)\n",
    "print('Best cross-validation accuracy:', grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07962af0-c60a-468d-b2f3-de2c458d6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Cross-Validation\n",
    "# Evaluate the best model using 5-fold cross-validation on the full dataset\n",
    "from sklearn.model_selection import cross_val_score\n",
    "best_rf = grid.best_estimator_  # Get the best model from GridSearchCV\n",
    "cv_scores = cross_val_score(best_rf, X, y, cv=5) # 5-fold CV scores\n",
    "print(f'Cross-Validation Accuracy Scores: {cv_scores}')\n",
    "print(f'Average CV Accuracy: {cv_scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27d540-3cdb-41d3-9bce-0c27cbc2f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Model Architecture Comparison\n",
    "# Purpose: Compare performance of different machine learning algorithms on the loan approval dataset\n",
    "from sklearn.linear_model import LogisticRegression # Linear classification model\n",
    "from xgboost import XGBClassifier # Gradient boosting model (optimized for performance)\n",
    "# Define a dictionary of models to compare:\n",
    " # 1. Logistic Regression (Baseline Linear Model)\n",
    "    # - Simple, interpretable, but assumes linear decision boundaries\n",
    "    # - max_iter=1000 ensures convergence (default may not suffice for some datasets)\n",
    " # 2. Random Forest (Best Tuned Model from GridSearchCV)\n",
    "    # - Ensemble of decision trees, handles non-linear relationships well\n",
    "    # - best_rf is the optimized model from earlier hyperparameter tuning\n",
    "# 3. XGBoost (Gradient Boosting)\n",
    "    # - State-of-the-art boosting algorithm, often high accuracy\n",
    "    # - use_label_encoder=False avoids warnings (uses native XGBoost label handling)\n",
    "    # - eval_metric='logloss' sets evaluation metric for binary classification\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000), # Linear model\n",
    "    'Random Forest': best_rf,  # Best tuned RF\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss') # Gradient boosting\n",
    "}\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train) # Fit the model to the training data\n",
    "    acc = model.score(X_test, y_test)  # Calculate accuracy on the test set\n",
    "    print(f'{name} Accuracy: {acc:.4f}') # Print formatted results: # - and .4f formats the accuracy to 4 decimal places\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Evaluate models using cross-validation and apply regularization.\n",
    "    \n",
    "    Why we do this:\n",
    "    - Cross-validation provides more reliable performance estimates\n",
    "    - Helps detect overfitting by evaluating on multiple validation sets\n",
    "    - Regularization prevents overfitting by penalizing complex models\n",
    "    - Ensures model generalizes well to unseen data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5ed66c-cf00-4ed1-84ff-ea095cd68555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
